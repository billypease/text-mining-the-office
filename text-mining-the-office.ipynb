{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting text from... http://officequotes.net/no1-01.php\n",
      "michael | 2\n",
      "pam | 0\n",
      "jim | 0\n",
      "andy | 16\n",
      "dwight | 1\n",
      "jan | 0\n",
      "ryan | 0\n",
      "oscar | 0\n",
      "stanley | 0\n",
      "kelly | 12\n",
      "toby | 4\n",
      "darryl | 16\n",
      "gabe | 2\n",
      "angela | 0\n",
      "robert | 0\n",
      "erin | 2\n",
      "holly | 4\n",
      "phyllis | 9\n",
      "meredith | 7\n",
      "pete | 0\n",
      "roy | 0\n",
      "creed | 5\n",
      "clark | 0\n",
      "charles | 0\n",
      "documentary crew member | 0\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import csv\n",
    "import urllib2\n",
    "import operator\n",
    "from os import path\n",
    "import collections\n",
    "import BeautifulSoup\n",
    "from wordcloud import WordCloud\n",
    "from stop_words import get_stop_words\n",
    "\n",
    "# Style guide: https://google.github.io/styleguide/pyguide.html\n",
    "# TODO(billypease@gmail.com): close open files (contextlib) or open \"with\"\n",
    "# TODO(billypease@gmail.com): make Stop Words its own function\n",
    "# TODO(billypease@gmail.com): run pylint over the code for formatting and bugs\n",
    "# TODO(billypease@gmail.com): implement Log Likelihood to Identify Characteristic Words\n",
    "# TODO(billypease@gmail.com): implement word counts and curse word counts per episode\n",
    "\n",
    "\n",
    "def get_page_text(url):\n",
    "    print \"Getting text from... \" + url\n",
    "    # given a url, get page content\n",
    "    data = urllib2.urlopen(url).read() \n",
    "    # parse as html structured document\n",
    "    bs = BeautifulSoup.BeautifulSoup(data, convertEntities=BeautifulSoup.BeautifulSoup.HTML_ENTITIES)\n",
    "    # find character quotes (e.g. <div class=\"quote\">) and extract text\n",
    "    txt = bs.findAll(attrs={\"class\": \"quote\"})\n",
    "    #txt = txt[0]\n",
    "    return txt\n",
    "\n",
    "\n",
    "def remove_noise(transcript):\n",
    "    # clean transcript; lower case it, remove tags and punctuation\n",
    "    noise = {'<br />': '', '<div class=\"quote\">': '', '<b>': '', '</b>': '', \n",
    "                '</div>': '', '[': '', ']': '', '.': '', '?': '', ',': '',\n",
    "                '!': '', '\"': '', '<u>': '', '</u>': '', '<div class=\"spacer\">': '',\n",
    "                'deleted scene': '', \"'\": ''}\n",
    "    cleanedt = str(transcript).lower()\n",
    "    for x,y in noise.items():\n",
    "        cleanedt = cleanedt.replace(x, y)\n",
    "    return cleanedt\n",
    "\n",
    "\n",
    "def print_to_text_file(txt, filename):\n",
    "    # write text to a specified file\n",
    "    text_file = open(filename, \"w\")\n",
    "    text_file.write(txt)\n",
    "    text_file.close()\n",
    "    return\n",
    "\n",
    "\n",
    "def print_user_lines_to_file(source_file, out_file, tv_character_name):\n",
    "    with open(out_file, 'a') as f1:\n",
    "        for line in open(source_file):\n",
    "            if line.startswith(tv_character_name): f1.write(line)\n",
    "    return\n",
    "\n",
    "\n",
    "def character_word_analysis(t_out_file, t_count_file, main_character):\n",
    "    # count words in tv character's transcript file\n",
    "    words = collections.Counter()    \n",
    "    char_transcript = open(t_out_file)\n",
    "    for line in char_transcript:\n",
    "        words.update(line.split())\n",
    "    # store character's word counts in a CSV file\n",
    "    word_count_file = open(t_count_file, 'w')\n",
    "    writer = csv.writer(word_count_file)\n",
    "    for word, count in words.iteritems():\n",
    "        # don't count starting word name: (e.g. \"pam:\", \"jim:\", etc.)\n",
    "        if word != main_character + ':':\n",
    "            # CSV row: tv character name, word, count\n",
    "            writer.writerow([main_character, word, count])\n",
    "    return\n",
    "\n",
    "\n",
    "def sum_word_count(count_file, subject):\n",
    "    cr = csv.reader(open(count_file,\"rb\"))\n",
    "    total_count = sum(int(x[2]) for x in cr)\n",
    "    #print '%s | %d' % (subject, total_count)\n",
    "    return\n",
    "\n",
    "\n",
    "def sum_curse_word_count(count_file, subject):\n",
    "    bad_words = {'damn', 'bitch', 'ass', 'hell', 'shit', 'whore'}\n",
    "    curse_count = 0\n",
    "    cr2 = csv.reader(open(count_file,\"rb\"))\n",
    "    for y in cr2:\n",
    "        if str(y[1]) in bad_words:\n",
    "            curse_count = curse_count + int(y[2])\n",
    "    print '%s | %d' % (subject, curse_count)\n",
    "    return\n",
    "\n",
    "\n",
    "def make_cloud_tag(text):\n",
    "    # Generate a word cloud image\n",
    "    wordcloud = WordCloud().generate(text)\n",
    "    # take relative word frequencies into account, lower max_font_size\n",
    "    wordcloud = WordCloud(background_color='white', width=600, height = 400, \n",
    "                          max_font_size=60, relative_scaling=.5).generate(text)\n",
    "    # display image the pil way\n",
    "    image = wordcloud.to_image()\n",
    "    image.show()\n",
    "    return\n",
    "\n",
    "    \n",
    "def main():\n",
    "    \n",
    "    # The Office's main characters\n",
    "    main_characters = [\n",
    "        'michael', 'pam', 'jim', 'andy', 'dwight', 'jan', 'ryan', 'oscar', 'stanley',\n",
    "        'kelly', 'toby', 'darryl', 'gabe', 'angela', 'robert', 'erin', 'holly',\n",
    "        'phyllis', 'meredith', 'pete', 'roy', 'creed', 'clark', 'charles',\n",
    "        'documentary crew member'\n",
    "    ]\n",
    "\n",
    "    # get the URLs to scrape\n",
    "    with open(\"urls-for-testing.txt\") as f:\n",
    "    #with open(\"urls.txt\") as f:\n",
    "        urls = f.readlines()\n",
    "    transcript = [get_page_text(url) for url in urls]\n",
    "    \n",
    "    # clean transcript string of noise\n",
    "    cleaned_transcript = remove_noise(transcript)\n",
    "    \n",
    "    # remove Stop Words from cleaned_transcipt with regex\n",
    "    stop_words = get_stop_words('en')    \n",
    "    pattern = re.compile(r'\\b(' + r'|'.join(stop_words) + r')\\b\\s*')\n",
    "    cleaned_transcript = pattern.sub('', cleaned_transcript)\n",
    "    \n",
    "    # make a Cloud Tag from characters transcript\n",
    "    make_cloud_tag(cleaned_transcript)\n",
    "    \n",
    "    # print cleaned trancript string to text file\n",
    "    print_to_text_file(cleaned_transcript, \"the_office_transcripts.txt\")\n",
    "    \n",
    "    # count each word in transcript; store counts in all_count.csv\n",
    "    character_word_analysis('the_office_transcripts.txt', 'all_count.csv', 'all')\n",
    "    \n",
    "    # sum the number of words in all_count.csv\n",
    "    sum_word_count('all_count.csv', 'all')\n",
    "    \n",
    "    # get each of the main characters lines in the transcript\n",
    "    for main_character in main_characters:\n",
    "        # one txt file for transcript; one CSV file for word count\n",
    "        t_out_file = main_character + \".txt\"\n",
    "        t_count_file = main_character + \"_count.csv\"\n",
    "        \n",
    "        # tv character lines start with a :[name] (e.g. \"michael:\")\n",
    "        characters_line = main_character + \":\"\n",
    "        \n",
    "        # get lines for main TV characters and write them to their own txt file\n",
    "        print_user_lines_to_file(\"the_office_transcripts.txt\", t_out_file, characters_line)\n",
    "\n",
    "        # count each word for each TV character's transcript from txt file\n",
    "        character_word_analysis(t_out_file, t_count_file, main_character)\n",
    "        \n",
    "        # sum number of words each TV character said from CSV file\n",
    "        sum_word_count(t_count_file, main_character)\n",
    "        \n",
    "        sum_curse_word_count(t_count_file, main_character)\n",
    "        \n",
    "    print 'Done!'\n",
    "    \n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
